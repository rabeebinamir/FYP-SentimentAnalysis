{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> Natural Language Processing</h2> </center>\n",
    "<center> <h2> Text Classification using Scikit-Learn </h2> </center>\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Overview:</h3>\n",
    "\n",
    "<pre>\n",
    "The main objective of this project is to completely understand the text classification problem using Machine Learning toolkit (scikit-learn). In this project, we are going to use the unstructured dataset which consists of comments extracted from twitter. The dataset consist of input attribute (Comment) and ouput attribute (Offensive). Dataset is labelled. The task is to predict offensive comment by using Machine Learning algorithms. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Machine Learning Approach:</h3>\n",
    "\n",
    "<pre>\n",
    "The problem of Text Classification is treated as a supervised learning approach because we going to use the labelled dataset.</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The Input and Output are:</h3>\n",
    "\n",
    "<pre>\n",
    "<b> - Input: </b> Comment\n",
    "<b> - Output:</b> Offensive (1/0)\n",
    "<b> - Goal:  </b> Learn from Input to predict Output\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Three Phases of Machine Learning: </h3>\n",
    "\n",
    "<pre>\n",
    "<b>1. Training Phase </b> \n",
    "                    Learn from Training Dataset.\n",
    "<b>2. Testing/Validation/Evaluation Phase</b>\n",
    "                                         Evaluate how well the algorithm learned.\n",
    "<b>3. Application Phase</b>\n",
    "                       Use your learned/trained models in real world application.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2> Table of Contents </h2></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<h3>PHASE 1 & 2: TRAINING AND TESTING </h3>\n",
    "\n",
    "<pre>\n",
    "<b>Step 1:</b> Import Libraries\n",
    "<b>Step 2:</b> Read, Understand and Pre-process Train/Test Data\n",
    "       <b>2.1</b> Read Data\n",
    "       <b>2.2</b> Understand Data\n",
    "       <b>2.3</b> Pre-process Data\n",
    "       <b>2.4</b> Tokenization and Removal of Stop Words\n",
    "<b>Step 3:</b> Label Encoding for Train/Test Data\n",
    "<b>Step 4:</b> Feature Extraction – Changing Representation of features “from String to Feature-Vector”\n",
    "<b>Step 5:</b> Train Machine Learning Algorithms using Training Data\n",
    "<b>Step 6:</b> Evaluate Machine Learning Algorithms using Test Data\n",
    "<b>Step 7:</b> Selection of Best Model\n",
    "</pre>\n",
    "\n",
    "<h3> PHASE 3: APPLICATION PHASE</h3>\n",
    "\n",
    "<pre>\n",
    "<b>Step 8:</b> Application in Real World\n",
    "       <b>8.1</b> Combine Dataset (Train + Test)\n",
    "       <b>8.2</b> Train Best Model on all data (Train + Test)\n",
    "       <b>8.3</b> Save the Trained Model in Pickle File\n",
    "<b>Step 9:</b> Make predictions on unseen/new data\n",
    "       <b>9.1</b> Load the Trained Model\n",
    "       <b>9.2</b> Take Input from User\n",
    "       <b>9.3</b> Convert User Input into Feature Vector (Same as Feature Vector of Trained Model)\n",
    "       <b>9.4</b> Apply Trained Model on Feature Vector of Unseen Dataset and give Output Prediction to the User\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Step 1:</b> Import Libraries</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 2.76 seconds)\n",
      "\u001b[33mWriting emoji data to C:\\Users\\Rabee Bin Amir\\.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import scipy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from joblib import dump, load\n",
    "from astropy.table import Table, Column\n",
    "\n",
    "import urduhack\n",
    "from urduhack.preprocess import replace_urls\n",
    "from urduhack.preprocess import replace_numbers\n",
    "from urduhack.preprocess import remove_punctuation\n",
    "from urduhack.preprocess import remove_accents\n",
    "from urduhack.preprocess import remove_english_alphabets\n",
    "from urduhack.preprocess import normalize_whitespace\n",
    "from urduhack.normalization import normalize\n",
    "#from urduhack.tokenization import word_tokenizer\n",
    "\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Step 2:</b> Read, Understand and Pre-process Train/Test Data</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><b>Step 2.1: Read Data </b></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"fyp_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><b>Step 2.2: Understand Data </b></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of instances in Dataset:        10138\n",
      "==============================\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "dataset.columns.name = 'index'\n",
    "\n",
    "print('Number of instances in Dataset:        %d'%(len(dataset)))\n",
    "print('==============================',end='\\n\\n')\n",
    "\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><b>Step 2.3: Pre-Process Data </b></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "def clean_comment(comment):\n",
    "    \n",
    "    comment = demoji.replace(comment,\"\")\n",
    "    \n",
    "    comment = re.sub(r'@[A-Za-z0-9]+', '', comment)\n",
    "    \n",
    "    comment = re.sub(r'#[A-Za-z0-9]*', '', comment)\n",
    "    \n",
    "    comment = replace_urls(comment,replace_with=\"\")\n",
    "    \n",
    "    comment = replace_numbers(comment,replace_with=\"\")\n",
    "    \n",
    "    comment = remove_accents(comment)\n",
    "    \n",
    "    comment = remove_english_alphabets(comment)\n",
    "    \n",
    "    comment = normalize_whitespace(comment)\n",
    "    \n",
    "    comment = remove_punctuation(comment)\n",
    "    \n",
    "    comment = normalize(comment)\n",
    "    \n",
    "    return comment\n",
    "\n",
    "for i in dataset.index:\n",
    "    a = dataset['comment'][i] \n",
    "    b = clean_comment(a)\n",
    "    dataset.loc[[i],['comment']] = b\n",
    "    \n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><b>Step 2.4: Tokenization and Removal of Stop Words </b></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(\"\"\"\n",
    "\n",
    " آ آئی آئیں آئے آتا آتی آتے آداب آدھ آدھا آدھی آدھے آس آمدید آنا آنسہ آنی آنے\n",
    " آپ آگے آہ آہا آیا اب ابھی ابے اتوار ارب اربویں ارے اس اسکا اسکی اسکے اسی اسے اف\n",
    " افوہ الاول البتہ الثانی الحرام السلام الف المکرم ان اندر انکا انکی انکے انہوں انہی انہیں اوئے اور\n",
    " اوپر اوہو اپ اپنا اپنوں اپنی اپنے اپنےآپ اکبر اکثر اگر اگرچہ اگست اہاہا ایسا ایسی ایسے ایک بائیں\n",
    " بار بارے بالکل باوجود باہر بج بجے بخیر برسات بشرطیکہ بعد بعض بغیر بلکہ بن بنا بناؤ بند\n",
    " بڑی بھر بھریں بھی بہار بہت بہتر بیگم تاکہ تاہم تب تجھ تجھی تجھے ترا تری تلک تم تمام\n",
    " تمہارا تمہاروں تمہاری تمہارے تمہیں تو تک تھا تھی تھیں تھے تہائی تیرا تیری تیرے تین جا جاؤ\n",
    " جائیں جائے جاتا جاتی جاتے جانی جانے جب جبکہ جدھر جس جسے جن جناب جنہوں جنہیں جو جہاں جی\n",
    " جیسا جیسوں جیسی جیسے جیٹھ حالانکہ حالاں حصہ حضرت خاطر خالی خدا خزاں خواہ خوب خود دائیں درمیان\n",
    " دریں دو دوران دوسرا دوسروں دوسری دوشنبہ دوں دکھائیں دگنا دی دیئے دیا دیتا دیتی دیتے دیر دینا دینی\n",
    " دینے دیکھو دیں دیے دے ذریعے رکھا رکھتا رکھتی رکھتے رکھنا رکھنی رکھنے رکھو رکھی رکھے رہ رہا\n",
    " رہتا رہتی رہتے رہنا رہنی رہنے رہو رہی رہیں رہے ساتھ سامنے ساڑھے سب سبھی سراسر سلام سمیت سوا\n",
    " سوائے سکا سکتا سکتے سہ سہی سی سے شام شاید شکریہ صاحب صاحبہ صرف ضرور طرح طرف طور\n",
    " علاوہ عین فروری فقط فلاں فی قبل قطا لئے لائی لائے لاتا لاتی لاتے لانا لانی لانے لایا لو\n",
    " لوجی لوگوں لگ لگا لگتا لگتی لگی لگیں لگے لہذا لی لیا لیتا لیتی لیتے لیکن لیں لیے\n",
    " لے ماسوا مت مجھ مجھی مجھے محترم محترمہ محترمی محض مرا مرحبا مری مرے مزید مس مسز مسٹر مطابق\n",
    " مطلق مل منٹ منٹوں مکرمی مگر مگھر مہربانی میرا میروں میری میرے میں نا نزدیک نما نو نومبر\n",
    " نہ نہیں نیز نیچے نے و وار واسطے واقعی والا والوں والی والے واہ وجہ ورنہ وعلیکم وغیرہ ولے\n",
    " وگرنہ وہ وہاں وہی وہیں ویسا ویسے ویں پاس پایا پر پس پلیز پون پونا پونی پونے پھاگن\n",
    " پھر پہ پہر پہلا پہلی پہلے پیر پیچھے چاہئے چاہتے چاہیئے چاہے چلا چلو چلیں چلے چناچہ چند چونکہ\n",
    " چوگنی چکی چکیں چکے چہارشنبہ چیت ڈالنا ڈالنی ڈالنے ڈالے کئے کا کاتک کاش کب کبھی کدھر کر\n",
    " کرتا کرتی کرتے کرم کرنا کرنے کرو کریں کرے کس کسی کسے کل کم کن کنہیں کو کوئی کون\n",
    " کونسا کونسے کچھ کہ کہا کہاں کہہ کہی کہیں کہے کی کیا کیسا کیسے کیونکر کیونکہ کیوں کیے\n",
    " کے گئی گئے گا گرما گرمی گنا گو گویا گھنٹا گھنٹوں گھنٹے گی گیا ہائیں ہائے ہاڑ ہاں ہر\n",
    " ہرچند ہرگز ہزار ہفتہ ہم ہمارا ہماری ہمارے ہمی ہمیں ہو ہوئی ہوئیں ہوئے ہوا ہوبہو ہوتا ہوتی\n",
    " ہوتیں ہوتے ہونا ہونگے ہونی ہونے ہوں ہی ہیلو ہیں ہے یا یات یعنی یک یہ یہاں یہی یہیں\n",
    "\n",
    "\n",
    "\"\"\".split())\n",
    "\n",
    "filtered_sentences = [] \n",
    "\n",
    "for i in dataset.index:\n",
    "    token = dataset['comment'][i].split()\n",
    "    for w in token: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentences.append(w) \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Step 3:</b> Label Encoding for Train/Test Data</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Step 4:</b> Feature Extraction</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "vector = TfidfVectorizer(strip_accents='unicode',\n",
    "                         analyzer = 'word',\n",
    "                         ngram_range = (1,3),\n",
    "                         max_features = 5000)\n",
    "\n",
    "all_data_features = vector.fit_transform(filtered_sentences)\n",
    "\n",
    "all_data_features_names = vector.get_feature_names()\n",
    "\n",
    "\n",
    "text1 = []\n",
    "for i in dataset.index:\n",
    "    text1.append(dataset[\"comment\"][i])\n",
    "X1 = vector.transform(text1)\n",
    "df1 = pd.DataFrame(X1.toarray(), columns = all_data_features_names)\n",
    "offensive1 = []\n",
    "for y in dataset.index:\n",
    "    offensive1.append(dataset[\"offensive\"][y])\n",
    "df1[\"offensive\"] = offensive1    \n",
    "df1.columns.name = 'index'\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Step 5:</b> Split Input Vectors and Labels</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X1\n",
    "y = dataset.offensive\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Step 5:</b> Train Machine Learning Algorithms using Training Data</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Parameters and their values:\n",
      "===========================\n",
      "\n",
      "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
      "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
      "                     max_iter=1000, multi_class='auto', n_jobs=None,\n",
      "                     penalty='l2', random_state=None, refit=True, scoring=None,\n",
      "                     solver='lbfgs', tol=0.0001, verbose=0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "logistic_regression_cv = LogisticRegressionCV(max_iter = 1000)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "logistic_regression_cv.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "print(\"Parameters and their values:\")\n",
    "print(\"===========================\",end='\\n\\n')\n",
    "print(logistic_regression_cv)\n",
    "model_names.append('LogisticRegressionCV')\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Parameters and their values:\n",
      "===========================\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "k_neighbors_classifier = KNeighborsClassifier()\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "k_neighbors_classifier.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "print(\"Parameters and their values:\")\n",
    "print(\"===========================\",end='\\n\\n')\n",
    "print(k_neighbors_classifier)\n",
    "model_names.append('KNeighborsClassifier')\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Parameters and their values:\n",
      "===========================\n",
      "\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "bernoulli_nb = BernoulliNB()\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "bernoulli_nb.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "print(\"Parameters and their values:\")\n",
    "print(\"===========================\",end='\\n\\n')\n",
    "print(bernoulli_nb)\n",
    "model_names.append('BernoulliNB')\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Parameters and their values:\n",
      "===========================\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier()\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "random_forest_classifier.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "print(\"Parameters and their values:\")\n",
    "print(\"===========================\",end='\\n\\n')\n",
    "print(random_forest_classifier)\n",
    "model_names.append('RandomForestClassifier')\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Parameters and their values:\n",
      "===========================\n",
      "\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=2000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "linear_svc = LinearSVC(max_iter = 2000)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "linear_svc.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "print(\"Parameters and their values:\")\n",
    "print(\"===========================\",end='\\n\\n')\n",
    "print(linear_svc)\n",
    "model_names.append('LinearSVC')\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Parameters and their values:\n",
      "===========================\n",
      "\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "gradient_boosting_classifier = GradientBoostingClassifier()\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "gradient_boosting_classifier.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "print(\"Parameters and their values:\")\n",
    "print(\"===========================\",end='\\n\\n')\n",
    "print(gradient_boosting_classifier)\n",
    "model_names.append('GradientBoostingClassifier')\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Parameters and their values:\n",
      "===========================\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "decision_tree_classifier.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "print(\"Parameters and their values:\")\n",
    "print(\"===========================\",end='\\n\\n')\n",
    "print(decision_tree_classifier)\n",
    "model_names.append('CART')\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Step 6:</b> Evaluate Machine Learning Algorithms using Test Data</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores1 = []\n",
    "Scores2 = []\n",
    "Scores3 = []\n",
    "Scores4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "y_prediction    = logistic_regression_cv.predict(X_test)\n",
    "\n",
    "Accuracy_Score  = round(accuracy_score(y_test,y_prediction),2)\n",
    "Precision_Score = round(precision_score(y_test, y_prediction),2)\n",
    "Recall_Score    = round(recall_score(y_test, y_prediction),2)\n",
    "F1_Score        = round(f1_score(y_test, y_prediction),2)\n",
    "\n",
    "Scores1.append(Accuracy_Score)\n",
    "Scores2.append(Precision_Score)\n",
    "Scores3.append(Recall_Score)\n",
    "Scores4.append(F1_Score)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "y_prediction    = k_neighbors_classifier.predict(X_test)\n",
    "\n",
    "Accuracy_Score  = round(accuracy_score(y_test,y_prediction),2)\n",
    "Precision_Score = round(precision_score(y_test, y_prediction),2)\n",
    "Recall_Score    = round(recall_score(y_test, y_prediction),2)\n",
    "F1_Score        = round(f1_score(y_test, y_prediction),2)\n",
    "\n",
    "\n",
    "Scores1.append(Accuracy_Score)\n",
    "Scores2.append(Precision_Score)\n",
    "Scores3.append(Recall_Score)\n",
    "Scores4.append(F1_Score)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "y_prediction    = bernoulli_nb.predict(X_test)\n",
    "\n",
    "Accuracy_Score  = round(accuracy_score(y_test,y_prediction),2)\n",
    "Precision_Score = round(precision_score(y_test, y_prediction),2)\n",
    "Recall_Score    = round(recall_score(y_test, y_prediction),2)\n",
    "F1_Score        = round(f1_score(y_test, y_prediction),2)\n",
    "\n",
    "Scores1.append(Accuracy_Score)\n",
    "Scores2.append(Precision_Score)\n",
    "Scores3.append(Recall_Score)\n",
    "Scores4.append(F1_Score)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "y_prediction    = random_forest_classifier.predict(X_test)\n",
    "\n",
    "Accuracy_Score  = round(accuracy_score(y_test,y_prediction),2)\n",
    "Precision_Score = round(precision_score(y_test, y_prediction),2)\n",
    "Recall_Score    = round(recall_score(y_test, y_prediction),2)\n",
    "F1_Score        = round(f1_score(y_test, y_prediction),2)\n",
    "\n",
    "\n",
    "Scores1.append(Accuracy_Score)\n",
    "Scores2.append(Precision_Score)\n",
    "Scores3.append(Recall_Score)\n",
    "Scores4.append(F1_Score)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "y_prediction    = linear_svc.predict(X_test)\n",
    "\n",
    "Accuracy_Score  = round(accuracy_score(y_test,y_prediction),2)\n",
    "Precision_Score = round(precision_score(y_test, y_prediction),2)\n",
    "Recall_Score    = round(recall_score(y_test, y_prediction),2)\n",
    "F1_Score        = round(f1_score(y_test, y_prediction),2)\n",
    "\n",
    "\n",
    "Scores1.append(Accuracy_Score)\n",
    "Scores2.append(Precision_Score)\n",
    "Scores3.append(Recall_Score)\n",
    "Scores4.append(F1_Score)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "y_prediction    = gradient_boosting_classifier.predict(X_test)\n",
    "\n",
    "Accuracy_Score  = round(accuracy_score(y_test,y_prediction),2)\n",
    "Precision_Score = round(precision_score(y_test, y_prediction),2)\n",
    "Recall_Score    = round(recall_score(y_test, y_prediction),2)\n",
    "F1_Score        = round(f1_score(y_test, y_prediction),2)\n",
    "\n",
    "Scores1.append(Accuracy_Score)\n",
    "Scores2.append(Precision_Score)\n",
    "Scores3.append(Recall_Score)\n",
    "Scores4.append(F1_Score)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "y_prediction    = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "Accuracy_Score  = round(accuracy_score(y_test,y_prediction),2)\n",
    "Precision_Score = round(precision_score(y_test, y_prediction),2)\n",
    "Recall_Score    = round(recall_score(y_test, y_prediction),2)\n",
    "F1_Score        = round(f1_score(y_test, y_prediction),2)\n",
    "\n",
    "Scores1.append(Accuracy_Score)\n",
    "Scores2.append(Precision_Score)\n",
    "Scores3.append(Recall_Score)\n",
    "Scores4.append(F1_Score)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Step 7:</b> Selection of Best Model</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Detailed Performance of all the models:\n",
      "=======================================\n",
      "+----------------------------+----------+-----------+--------+------+\n",
      "|           Model            | Accuracy | Precision | Recall |  F1  |\n",
      "+----------------------------+----------+-----------+--------+------+\n",
      "|    LogisticRegressionCV    |   0.93   |    0.91   |  0.95  | 0.93 |\n",
      "|    KNeighborsClassifier    |   0.79   |    0.86   |  0.71  | 0.77 |\n",
      "|        BernoulliNB         |   0.93   |    0.91   |  0.96  | 0.93 |\n",
      "|   RandomForestClassifier   |   0.94   |    0.91   |  0.97  | 0.94 |\n",
      "|         LinearSVC          |   0.92   |    0.91   |  0.93  | 0.92 |\n",
      "| GradientBoostingClassifier |   0.93   |    0.91   |  0.97  | 0.94 |\n",
      "|            CART            |   0.91   |    0.9    |  0.92  | 0.91 |\n",
      "+----------------------------+----------+-----------+--------+------+\n",
      "\n",
      "\n",
      "\n",
      "Best Model:\n",
      "=======================================\n",
      "+--------------------------+----------+\n",
      "|          Model           | Accuracy |\n",
      "+--------------------------+----------+\n",
      "|    LogisticRegressionCV    |   0.93   |\n",
      "+--------------------------+----------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "p = PrettyTable()\n",
    "p.field_names = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "p.add_row([model_names[0],Scores1[0],Scores2[0],Scores3[0],Scores4[0]])\n",
    "p.add_row([model_names[1],Scores1[1],Scores2[1],Scores3[1],Scores4[1]])\n",
    "p.add_row([model_names[2],Scores1[2],Scores2[2],Scores3[2],Scores4[2]])\n",
    "p.add_row([model_names[3],Scores1[3],Scores2[3],Scores3[3],Scores4[3]])\n",
    "p.add_row([model_names[4],Scores1[4],Scores2[4],Scores3[4],Scores4[4]])\n",
    "p.add_row([model_names[5],Scores1[5],Scores2[5],Scores3[5],Scores4[5]])\n",
    "p.add_row([model_names[6],Scores1[6],Scores2[6],Scores3[6],Scores4[6]])\n",
    "\n",
    "print('Detailed Performance of all the models:')\n",
    "print('=======================================')\n",
    "print(p)\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Best Model:')\n",
    "print('=======================================')\n",
    "print('+--------------------------+----------+')\n",
    "print('|          Model           | Accuracy |')\n",
    "print('+--------------------------+----------+')\n",
    "print('|    %s    |   %.2f   |'%(model_names[0],Scores1[0]))\n",
    "print('+--------------------------+----------+')\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Step 8: Application in Real World</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><b>8.1 Combine Dataset (Train + Test)</b></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "training_testing_dataset_after_fe = df1\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><b>8.2 Train Best Model on all data (Train + Test)</b></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
      "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
      "                     max_iter=1000, multi_class='auto', n_jobs=None,\n",
      "                     penalty='l2', random_state=None, refit=True, scoring=None,\n",
      "                     solver='lbfgs', tol=0.0001, verbose=0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "\n",
    "X = training_testing_dataset_after_fe.drop('offensive', axis=1)\n",
    "y = training_testing_dataset_after_fe['offensive']\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "logistic_regression_cv.fit(X,np.ravel(y))\n",
    "print(logistic_regression_cv)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><b>8.3 Save the Trained Model in Pickle File</b></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = pickle.dumps(logistic_regression_cv)\n",
    "saved_vector = pickle.dumps(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Step 9:</b> Make predictions on unseen/new data</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><b>9.1 Load the Trained Model</b></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestClassifier = pickle.loads(saved_model)\n",
    "BestVector = pickle.loads(saved_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre><b>9.2 Take Input from User</b></pre>\n",
    "<pre><b>9.3 Convert User Input into Feature Vector (Same as Feature Vector of Trained Model)</b></pre>\n",
    "<pre><b>9.4 Apply Trained Model on Feature Vector of Unseen Data and give Output Prediction to the User</b></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Tk()\n",
    "window.title(\"Twitter Sentiment Analysis\")\n",
    "window.geometry('1300x350')\n",
    "window.iconbitmap('Twitter-icon.ico')\n",
    "\n",
    "def prediction():\n",
    "    \n",
    "    input_comment = entry.get()\n",
    "    input_list = []\n",
    "    input_list.append(input_comment)\n",
    "    a = BestVector.transform(input_list)\n",
    "    b = pd.DataFrame(a.toarray(), columns = BestVector.get_feature_names())\n",
    "    prediction = BestClassifier.predict(b)\n",
    "    if prediction == 1:\n",
    "        output = \"offensive\"\n",
    "    else:\n",
    "        output = \"non-offensive\"\n",
    "    \n",
    "    newWindow = Toplevel(window)\n",
    "    newWindow.title(\"Prediction\")\n",
    "    newWindow.geometry('300x200')\n",
    "    newWindow.iconbitmap('Twitter-icon.ico')\n",
    "    finishButton = Button(newWindow, text=\"Exit\", padx=10, pady=10, font=\"Verdana 16 bold\", width=12 ,borderwidth=5, \\\n",
    "                  command=window.destroy)\n",
    "    finishButton.place(relx=.5, rely=.7, anchor=\"center\")\n",
    "    if (output == \"offensive\"):\n",
    "        labelOffensive = Label(newWindow, text = \"Offensive\", font=\"Verdana 20 bold\", bg=\"gray\", fg=\"red\")\n",
    "        labelOffensive.pack()\n",
    "    if (output == \"non-offensive\"):\n",
    "        labelNonOffensive = Label(newWindow, text = \"Non-Offensive\", font=\"Verdana 20 bold\", bg=\"gray\", fg=\"green\")\n",
    "        labelNonOffensive.pack()\n",
    "\n",
    "\n",
    "    \n",
    "labelFrame = LabelFrame(window, text=\"Enter the text for the prediction of gender:\", padx=35, pady=30, font=\"Verdana 16 bold\", \\\n",
    "                        borderwidth=10, height=20)\n",
    "labelFrame.grid(row=0, column=0, columnspan=3)\n",
    "labelFrame.place(relx=.5, rely=.3, anchor=\"center\")\n",
    "\n",
    "entry = Entry(labelFrame,width=78, borderwidth=5, justify=\"left\", font=\"Verdana 16 bold\")\n",
    "entry.grid(row=1, column=0, columnspan=3, ipady=5)\n",
    "\n",
    "btn1 = Button(window, text=\"Predict\", padx=10, pady=10, font=\"Verdana 16 bold\", width=12, \\\n",
    "              borderwidth=5, command=prediction)\n",
    "btn1.place(relx=.2, rely=.7, anchor=\"center\")\n",
    "\n",
    "btn2 = Button(window, text=\"Exit\", padx=10, pady=10, font=\"Verdana 16 bold\", width=12 ,borderwidth=5, command=window.destroy)\n",
    "btn2.place(relx=.8, rely=.7, anchor=\"center\")\n",
    "\n",
    "\n",
    "\n",
    "mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
